{
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "481a9a63",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ef08dc2ed6f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "class Results:\n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                html = requests.get(url)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text] * len(df)\n",
    "\n",
    "                #馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "            #存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            #Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "    race_id_list = []\n",
    "    \n",
    "for place in range(1, 11, 1):\n",
    "    for kai in range(1, 13, 1):\n",
    "        for day in range(1, 13, 1):\n",
    "            for r in range(1, 13, 1):\n",
    "                race_id = \"2019\" + str(place).zfill(2) + str(kai).zfill(2) + str(day).zfill(2) + str(r).zfill(2)\n",
    "                race_id_list.append(race_id)\n",
    "\t\t\n",
    "results = Results.scrape(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorseResults:\n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        horse_results = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "                df = pd.read_html(url)[3]\n",
    "                #受賞歴がある馬の場合、3番目に受賞歴テーブルが来るため、4番目のデータを取得する\n",
    "                if df.columns[0]=='受賞歴':\n",
    "                    df = pd.read_html(url)[4]\n",
    "                df.index = [horse_id] * len(df)\n",
    "                horse_results[horse_id] = df\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる        \n",
    "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "\n",
    "        return horse_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#血統データを処理するクラス\n",
    "class Peds:\n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        \"\"\"\n",
    "        血統データをスクレイピングする関数\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        horse_id_list : list\n",
    "            馬IDのリスト\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        peds_df : pandas.DataFrame\n",
    "            全血統データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "\n",
    "        peds_dict = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                #重複を削除して1列のSeries型データに直す\n",
    "                generations = {}\n",
    "                for i in reversed(range(5)):\n",
    "                    generations[i] = df[i]\n",
    "                    df.drop([i], axis=1, inplace=True)\n",
    "                    df = df.drop_duplicates()\n",
    "                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "\n",
    "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #列名をpeds_0, ..., peds_61にする\n",
    "        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n",
    "\n",
    "        return peds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#払い戻し表データを処理するクラス\n",
    "class Return:\n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        \"\"\"\n",
    "        払い戻し表データをスクレイピングする関数\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        race_id_list : list\n",
    "            レースIDのリスト\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        return_tables_df : pandas.DataFrame\n",
    "            全払い戻し表データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "\n",
    "                #普通にスクレイピングすると複勝やワイドなどが区切られないで繋がってしまう。\n",
    "                #そのため、改行コードを文字列brに変換して後でsplitする\n",
    "                f = urlopen(url)\n",
    "                html = f.read()\n",
    "                html = html.replace(b'<br />', b'br')\n",
    "                dfs = pd.read_html(html)\n",
    "\n",
    "                #dfsの1番目に単勝〜馬連、2番目にワイド〜三連単がある\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "\n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        return return_tables_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(old, new):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    old : pandas.DataFrame\n",
    "        古いデータ\n",
    "    new : pandas.DataFrame\n",
    "        新しいデータ\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    \n",
    "    return pd.concat([filtered_old, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"    \n",
    "    Attributes:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        rawデータ\n",
    "    data_p : pd.DataFrame\n",
    "        preprocessing後のデータ\n",
    "    data_h : pd.DataFrame\n",
    "        merge_horse_results後のデータ\n",
    "    data_pe : pd.DataFrame\n",
    "        merge_peds後のデータ\n",
    "    data_c : pd.DataFrame\n",
    "        process_categorical後のデータ\n",
    "    no_peds: Numpy.array\n",
    "        merge_pedsを実行した時に、血統データが存在しなかった馬のhorse_id一覧\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.data_p = pd.DataFrame()\n",
    "        self.data_h = pd.DataFrame()\n",
    "        self.data_pe = pd.DataFrame()\n",
    "        self.data_c = pd.DataFrame()\n",
    "        \n",
    "    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n",
    "        \"\"\"\n",
    "        馬の過去成績データから、\n",
    "        n_samples_listで指定されたレース分の着順と賞金の平均を追加してdata_hに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        hr : HorseResults\n",
    "            馬の過去成績データ\n",
    "        n_samples_list : list, default [5, 9, 'all']\n",
    "            過去何レース分追加するか\n",
    "        \"\"\"\n",
    "    　　\n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_samples in n_samples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "\t\n",
    "\t#6/6追加： 馬の出走間隔追加\n",
    "\tself.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days\n",
    "        self.data_h.drop(['開催', 'latest'], axis=1, inplace=True)\n",
    "\t\n",
    "\t    \n",
    "    def merge_peds(self, peds):\n",
    "        \"\"\"\n",
    "        5世代分血統データを追加してdata_peに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        peds : Peds.peds_e\n",
    "            Pedsクラスで加工された血統データ。\n",
    "        \"\"\"\n",
    "\t\n",
    "        self.data_pe = \\\n",
    "            self.data_h.merge(peds, left_on='horse_id', right_index=True,\n",
    "                                                             how='left')\n",
    "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]\\\n",
    "            ['horse_id'].unique()\n",
    "        if len(self.no_peds) > 0:\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "            \n",
    "    def process_categorical(self, le_horse, le_jockey, results_m):\n",
    "        \"\"\"\n",
    "        カテゴリ変数を処理してdata_cに返す\n",
    "        Parameters:\n",
    "        ----------\n",
    "        le_horse : sklearn.preprocessing.LabelEncoder\n",
    "            horse_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "        le_jockey : sklearn.preprocessing.LabelEncoder\n",
    "            jockey_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "        results_m : Results.data_pe\n",
    "            ダミー変数化のとき、ResultsクラスとShutubaTableクラスで列を合わせるためのもの\n",
    "        \"\"\"\n",
    "\t\n",
    "        df = self.data_pe.copy()\n",
    "        \n",
    "        #ラベルエンコーディング。horse_id, jockey_idを0始まりの整数に変換\n",
    "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
    "        \n",
    "        #horse_id, jockey_idをpandasのcategory型に変換\n",
    "        df['horse_id'] = df['horse_id'].astype('category')\n",
    "        df['jockey_id'] = df['jockey_id'].astype('category')\n",
    "        \n",
    "        #そのほかのカテゴリ変数をpandasのcategory型に変換してからダミー変数化\n",
    "        #列を一定にするため\n",
    "        weathers = results_m['weather'].unique()\n",
    "        race_types = results_m['race_type'].unique()\n",
    "        ground_states = results_m['ground_state'].unique()\n",
    "        sexes = results_m['性'].unique()\n",
    "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n",
    "        df['性'] = pd.Categorical(df['性'], sexes)\n",
    "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])\n",
    "        \n",
    "        self.data_c = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "        \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        \"\"\"\n",
    "        レース結果データをスクレイピングする関数\n",
    "        Parameters:\n",
    "        ----------\n",
    "        race_id_list : list\n",
    "            レースIDのリスト\n",
    "        Returns:\n",
    "        ----------\n",
    "        race_results_df : pandas.DataFrame\n",
    "            全レース結果データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "\n",
    "        #race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "                #メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                html = requests.get(url)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "                #天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[0])] * len(df)\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text] * len(df)\n",
    "\n",
    "                #馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "                time.sleep(1)\n",
    "            #存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            #wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            #Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "    \n",
    "    #前処理    \n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['rank'] = df['着順'].map(lambda x:1 if x<4 else 0)\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0]\n",
    "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
    "\t\n",
    "\t#errors='coerce'で、\"計不\"など変換できない時に欠損値にする\n",
    "\tdf['体重'] = pd.to_numeric(df['体重'], errors='coerce')\n",
    "\tdf['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n",
    "\n",
    "        # 単勝をfloatに変換\n",
    "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "\t# 距離は10の位を切り捨てる\n",
    "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df.drop([\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\", '馬名', '騎手', '人気', '着順'],\n",
    "                axis=1, inplace=True)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
    "        \n",
    "        #開催場所\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "\t\n",
    "\t#6/6出走数追加\n",
    "        df['n_horses'] = df.index.map(df.index.value_counts())\n",
    "\n",
    "        self.data_p = df\n",
    "    \n",
    "    #カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n",
    "        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])\n",
    "        super().process_categorical(self.le_horse, self.le_jockey, self.data_pe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd00158366a7b2e88a54dd1e055df81182a330743eeb06806d8531d70a1513cdaad",
   "display_name": "Python 3.8.8 64-bit ('flaskenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0158366a7b2e88a54dd1e055df81182a330743eeb06806d8531d70a1513cdaad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}